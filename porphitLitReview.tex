\documentclass[conference]{IEEEtran}
% \IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\loIr.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{PPOrphIt: PPO Robot Manipulator Morphology Design \\ Literature Review}

\author{\IEEEauthorblockN{Steve Gillet}
\IEEEauthorblockA{\textit{University of Colorado Boulder Robotics} \\
\textit{Email: steve.gillet@colorado.edu}}
}

\maketitle

\begin{abstract}
This paper aims to address the cost and overactuation of robotics used in manufacturing by leveraging reinforcement learning (RL) to optimize robot morphologies to specific use cases.
RL is the right tool for this job because it can optimize over the large search space of possible robot morphologies and effectively explore large varieties of embodiments.
The core methodology involves using a PPO model to iterate on the different morphologies and MuJoCo to evaluate their effectiveness in simulation.
The evaluation signals will be success, path and energy efficiency, simplicity and manipulability index and these will be used to show the effectiveness of the PPO model at creating morphologies that are faster, more effective, and more efficient for particular tasks.
\end{abstract}

\begin{IEEEkeywords}
reinforcement learning, robot morphology, manipulator design
\end{IEEEkeywords}

\section{Introduction and Motivation}
High costs, lack of tailorability, and lack of retrofitting are some of the most cited problems for the adoption of the robotics in industrial settings \cite{mckinsey2019industrial}.
The most commonly used robotic manipulator in manufacturing is FANUC series of 6-DOF manipulators which cost from \$17,500 to \$400,000 depending on their capacity and customizations \cite{standardbots2025fanucprice}\cite{patentpc2025toprobotics}.
Meanwhile \cite{russo2021task} and \cite{he2019underactuated} show that most manipulators are overactuated, less cost-efficient, and less effective for most tasks they are used in.
I want to address this problem by creating an automation pipeline where you feed in a use case and you get out a robot specifically optimized for that use case.
By matching the robot to the task I can ensure that each robot can perform its task effectively and only the parts needed for that task are used, reducing cost and increasing efficiency.
Here I describe the process by which robot manipulator morphologies are optimized to particular tasks using a PPO agent.

I chose reinforcement learning because morphology design is an optimization problem over a complex, high-dimensional search space and RL can explore and learn optimal designs through trial-and-error in simulation \cite{sutton1988learning}.
I chose PPO because it is robust and on-policy and excels in continuous action spaces which is ideal for morphology design where there are continuous parameters like link lengths \cite{schulman2017proximal}.
This is more effective than imitation learning because imitation learning can only use morphologies that already exist severely limiting sample space and innovation \cite{delgado2022robotics}.
Supervised learning suffers from a very similar issue where labelled data would have to be provided which would be infeasible and there is no mechanism for exploration \cite{kober2013reinforcement}.

The core method is to use stable\_baselines3 PPO implementation to iterate on the design parameters (number of links, joint types (X, Y, Z hinge, and Z actuation), and link lengths), use MuJoCo to simulate that iteration on a simple task using RRTConnect and damped least squares (DLS) inverse kinematics (IK) for the path planning, feed the results (success, efficiency, and manipulability index) back into the PPO model.
I chose RRTConnect because it is simple and has been shown to be faster than other methods \cite{844730} which enables faster and more robust evaluation steps.
I also chose DLS IK for a similar reason, it has been shown to be robust especially in situations where the target position is out of reach \cite{buss2004introduction} which is inevitable in a situation where an agent is exploring morphologies.

The evaluation signals will be successful path planning, path length, energy cost, and manipulability index.
The manipulability index is a measure of a manipulators ability to arbitrarily change the position and orientation of the end effector at a particular point, borrowed from \cite{yoshikawa1985manipulability}.
The hope is that the better morphologies will be able to do the tasks more successfully, efficiently, and quickly while still maintaing some robustness and then I can vary the tasks slightly to find better morphologies for particular tasks.
The tasks will be kept simple but varied in ways to test different types of movements like moving near the base, moving from near to far, different elevations, more linear movements.
You can imagine that a lot of pick and place tasks are mostly linear and so might largely be done by linear actuators more efficiently.

The research questions I seek to answer: Can PPO be used to generate more efficient morphologies? What are the best evaluation signals to use? How does varying evaluation signals yield different results (does optimizing for path shortness yield more simple manipulators while optimizing for manipulability measure yield more complex manipulators?)?

\section{Background and Related Work}
Most of the research in this area codesigns morphology and control of modular, atomic robots like \cite{bhatia2023reinforcement} which uses PPO, \cite{tjanaka2023co} which uses TD3, \cite{spielberg2025accelerated}, and \cite{kalimuthu2023} which uses PPO and A3C for configuration and control of a reconfigurable, modular robot.
More pertinent \cite{ding2024modular} uses DDQN to co-optimize morphology and control for modular robotic manipulators and \cite{luck2019data} which uses soft actor critic for morphology and control of quadraped robots to avoid evalutating performance in sim or real to save time.
My method will focus on manipulators with simple controllers to allow me to focus on morphology design.
The method will use standard 6-DOF FANUC and 7-DOF Franka Emika Panda morphology as a baseline and contrast with PPO generated morphologies.
PPO has been used effectively in some of the relevant research and allows for searching over the continuous space of manipulator parameters instead of the discrete space of different module types in the atomic, reconfigurable manipulator optimization.
In the future, I would also like to try other RL approaches like Dreamer.
Dreamer uses a 'world model' to predict outcomes and plan ahead which improves sample efficiency which might be even more effective by reducing computation needs \cite{hafner2019dream}.

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}